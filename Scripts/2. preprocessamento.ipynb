{"cells":[{"cell_type":"markdown","metadata":{"id":"mpF8ARyRHCP7"},"source":["# Importações"]},{"cell_type":"code","source":["!pip install Unidecode\n","!pip install -U spacy\n","!python -m spacy download pt_core_news_sm"],"metadata":{"id":"BrCtMZnP7WMy","executionInfo":{"status":"ok","timestamp":1693949753575,"user_tz":180,"elapsed":44772,"user":{"displayName":"NICOLLY LIRA DE ALBUQUERQUE","userId":"13949517078806762460"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e63ddae-565b-436f-b46a-f5c59351cc0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n","2023-09-05 21:35:39.565506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-09-05 21:35:41.661964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting pt-core-news-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.6.0/pt_core_news_sm-3.6.0-py3-none-any.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.6.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_sm')\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5wJcEDQHBfR","outputId":"1ff0eb38-7667-4c97-b446-e4066e743184","executionInfo":{"status":"ok","timestamp":1693949769818,"user_tz":180,"elapsed":16270,"user":{"displayName":"NICOLLY LIRA DE ALBUQUERQUE","userId":"13949517078806762460"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","import re\n","import ast\n","import spacy\n","import nltk\n","from nltk.corpus import stopwords\n","import matplotlib.pyplot as plt\n","from unidecode import unidecode\n","from wordcloud import WordCloud\n","\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"NPwv1f8eHG_1"},"source":["# Definição de funções"]},{"cell_type":"markdown","metadata":{"id":"B0GrTXSFHZOG"},"source":["### Pré processamento do léxico"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LR-SAqvkHBYO"},"outputs":[],"source":["def lexicon_preprocess(lexicon_file, regex):\n","\n","  '''Read a txt file with the depression lexicon, tokenize by transforming the\n","      file into a python list. Preprocess by deleting ponctuation, acentuation\n","      and save the file in csv format.'''\n","\n","  with open(lexicon_file, \"r\", encoding='latin1') as file:\n","    lexicon = file.read().split(',')\n","\n","    lexicon = list(map(lambda x: re.sub(regex, ' ', x), lexicon))\n","    lexicon = [word.split(' ') for word in lexicon]\n","    lexicon = [[word for word in sublists if word != ''] for sublists in lexicon]\n","    df_lexicon = pd.DataFrame({'terms': lexicon})\n","\n","   # return df_lexicon.to_csv('lexicon_preprocess.csv', encoding = 'utf8', sep = ',', index=False)\n","  return df_lexicon"]},{"cell_type":"markdown","metadata":{"id":"8RLHlbqbHPhO"},"source":["### Uniformização dos submissions\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJN8CKaIHBcd"},"outputs":[],"source":["def submissions_standardization(submissions_file, title_column, text_column, regex):\n","\n","    '''Read a csv file with reddit submissions, transform into a pandas dataframe,\n","       get the texts columns (title and self post/ self text) and unify both columns.\n","       Then preprocess the unified text column by transforming all the text in lower\n","       case, deleting ponctuation, acentuation, emojis and, at the and, tokenize the\n","       text and save all file in csv format again with the new preprocessed column.'''\n","\n","    df_submissions = pd.read_csv(submissions_file, sep = ',')\n","\n","    df_submissions[title_column] = df_submissions[title_column].apply(lambda x: str(x))\n","    df_submissions[text_column] = df_submissions[text_column].apply(lambda x: str(x))\n","\n","    df_submissions[\"full_text\"] = df_submissions[[title_column, text_column]].apply(\" \".join, axis =1)\n","    df_submissions[\"full_text\"] = df_submissions[\"full_text\"].apply(lambda x: re.sub(regex, ' ', unidecode(x.lower())))\n","    df_submissions[\"full_text\"] = df_submissions[\"full_text\"].apply(lambda x: x.split(' '))\n","\n","    #return df_submissions.to_csv('submissions_preprocessed.csv',\n","    #                             encoding = 'utf8', sep = ',', index=False)\n","    return df_submissions"]},{"cell_type":"markdown","source":["### Traduzir gírias da internet para o português formal"],"metadata":{"id":"Z3F9Y5wV4IDG"}},{"cell_type":"code","source":["def internet_to_portuguese(df, column):\n","  internt_text_dict = {\n","    'vc': 'voce',\n","    'pq': 'porque',\n","    'blz': 'beleza',\n","    'tbm': 'tambem',\n","    'td': 'tudo',\n","    'flw': 'falou',\n","    'gnt': 'gente',\n","    'qnd': 'quando',\n","    'qd': 'quando',\n","    'vdd': 'verdade',\n","    'mt': 'muito',\n","    'ctz': 'certeza',\n","    'bjo': 'beijo',\n","    't+': 'ate mais',\n","    'tb': 'tambem',\n","    'nt': 'nao',\n","    'n' : 'nao',\n","    'pqna': 'pequena',\n","    'cmg': 'comigo',\n","    'qdo': 'quando',\n","    'dps': 'depois',\n","    'msm': 'mesmo',\n","    'mta': 'muita',\n","    'mtos': 'muitos',\n","    'vlw': 'valeu',\n","    'dsclp': 'desculpa',\n","    'nd': 'nada',\n","    'bj': 'beijo',\n","    'qse': 'quase',\n","    'sqn': 'só que nao',\n","    'tbém': 'tambem',\n","    'flws': 'falou',\n","    'eh': 'e',\n","    'obg': 'obrigado',\n","    'kk': 'risos',\n","    'vl': 'vale',\n","    'fzr': 'fazer',\n","    'pfv': 'por favor',\n","    'sq': 'só que',\n","    'ngm': 'ninguem',\n","    'sdds': 'saudades',\n","    'bomdia': 'bom dia',\n","    'bm': 'bem',\n","    'tdb': 'tudo bem',\n","    'eae': 'e ai',\n","    'qto': 'quanto',\n","    'to': 'estou',\n","    'qnd': 'quando',\n","    'cm': 'com',\n","    'q': 'que',\n","    'dsclpa': 'desculpa',\n","    'aff': 'exasperacao',\n","    'agr': 'agora',\n","    'mtos': 'muitos',\n","    'bjss': 'beijos',\n","    'vamo': 'vamos',\n","    'mtu': 'muito',\n","    'mt' : 'muito',\n","    'axo': 'acho',\n","    'td': 'tudo',\n","    'smp': 'sempre',\n","    'qqr': 'qualquer',\n","    'fzd': 'fazendo',\n","    'tmj': 'tamo junto',\n","    'qdo': 'quando',\n","    'qq': 'qualquer',\n","    'pf': 'por favor',\n","    'bomdia': 'bom dia',\n","    'sdd': 'saudade',\n","    't+': 'ate mais',\n","    'gnt': 'gente',\n","    'nd': 'nada',\n","    's': 'sim',\n","    'so': 'so',\n","    'soq': 'so que',\n","    'qdo': 'quando',\n","    'xau': 'tchau',\n","    'obg': 'obrigado',\n","    'msm': 'mesmo'\n","  }\n","  clean_column = df[column].apply(lambda x: [internt_text_dict[word] if word in internt_text_dict else word for word in x])\n","\n","  return clean_column"],"metadata":{"id":"XEIcr_Rk4eNF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Contagem de palavras do léxico por submission"],"metadata":{"id":"3GezYThF4ycR"}},{"cell_type":"code","source":["def check_for_term(df_lexico, column_lexico, df_submissions, column_submissions):\n","  total_list = []\n","  for text in df_submissions[column_submissions]:\n","    qtd_total = 0\n","    for term in df_lexico[column_lexico]:\n","      if term[0] not in text:\n","        boolean = 0\n","      else:\n","        if len(term) == 1:\n","          boolean = 1\n","        elif len(term) > 1:\n","          idx = text.index(term[0])\n","          qtd_term = 0\n","          for index_in_term in range(len(term)):\n","            if len(text) >= (idx+len(term)):\n","              for index_in_text in range(idx, (idx+len(term))):\n","                if term[index_in_term] == text[index_in_text]:\n","                  qtd_term = qtd_term + 1\n","                  #print(qtd_term)\n","            else:\n","              pass\n","          if qtd_term == len(term):\n","            boolean = 1\n","          else:\n","            boolean = 0\n","      qtd_total = qtd_total + boolean\n","    total_list.append(qtd_total)\n","  return total_list"],"metadata":{"id":"2zbXB3d440E9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Separar df_amostra e df_controle"],"metadata":{"id":"G-o4kPw548ol"}},{"cell_type":"code","source":["def split_groups(df, column, n_cutoff):\n","  df_sample = df[df[column] > n_cutoff]\n","  df_sample.reset_index(drop=True, inplace = True)\n","  df_control = df[df[column] <= n_cutoff]\n","  df_control.reset_index(drop=True, inplace = True)\n","\n","  return df_sample, df_control"],"metadata":{"id":"_yAOFE-5-zv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Stop Words"],"metadata":{"id":"d63lFSLr-1rj"}},{"cell_type":"markdown","source":["#### Create"],"metadata":{"id":"HFhQ7VLFBSpD"}},{"cell_type":"code","source":["def create_stopwords (df, column, contextual_stopwords):\n","  words_by_pos = []\n","  spacy_pt = spacy.load(\"pt_core_news_sm\")\n","  for text_list in df[column]:\n","    for word in text_list:\n","      doc = spacy_pt(word)\n","      for token in doc:\n","        if token.pos_ not in ('NOUN', 'ADJ'):\n","          words_by_pos.append(token.text)\n","        else:\n","          pass\n","\n","\n","  stop_words = list(stopwords.words('portuguese'))\n","  stop_words.extend(list(tuple(words_by_pos)))\n","  stop_words.extend(contextual_stopwords)\n","  return stop_words"],"metadata":{"id":"JIqlBmMKBC2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Remove"],"metadata":{"id":"AiyEgB7tBVN-"}},{"cell_type":"code","source":["def remove_stopwords (df, column, stop_words):\n","  clean_text = []\n","  spacy_pt = spacy.load(\"pt_core_news_sm\")\n","  for text_list in df[column]:\n","    line_list = []\n","    for word in text_list:\n","      doc = spacy_pt(word)\n","      for token in doc:\n","        if token.text not in stop_words:\n","          line_list.append(token.lemma_)\n","    clean_text.append(line_list)\n","\n","  return clean_text"],"metadata":{"id":"8TKLcnQtBUug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdsgibXLHpg6"},"source":["# Definição de constantes"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1qidLDvIHOo","executionInfo":{"status":"ok","timestamp":1693949772365,"user_tz":180,"elapsed":2137,"user":{"displayName":"NICOLLY LIRA DE ALBUQUERQUE","userId":"13949517078806762460"}},"outputId":"7a33886a-3173-4da0-ec20-89dec35d5e7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wA_g91tHpE3"},"outputs":[],"source":["submissions_file = \"/content/drive/MyDrive/tcc/bases_de_dados/bases_cruas/submissions_total_2023_04_06.csv\"\n","lexicon_file = \"/content/drive/MyDrive/tcc/bases_de_dados/bases_cruas/lexico_traduzido.txt\"\n","regex = r'[^a-zA-Z0-9À-ÿ\\x200b]|_+|\\n|x200b'"]},{"cell_type":"markdown","metadata":{"id":"jvxNAxenHxDa"},"source":["# Utilização de funções"]},{"cell_type":"code","source":["# Lexico\n","clean_lexicon = lexicon_preprocess(lexicon_file, regex)"],"metadata":{"id":"jMCQAmZb5mEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7qsolAbG8cB"},"outputs":[],"source":["# Uniformização dos textos\n","df_submissions = submissions_standardization(submissions_file, 'title', 'selftext', regex)"]},{"cell_type":"code","source":["#Traduzir gírias da internet\n","translate_column = internet_to_portuguese(df_submissions, 'full_text')\n","df_submissions['full_text'] = translate_column"],"metadata":{"id":"tcxDRyT49rTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Contar palavras do lexico no texto\n","qtd_list = check_for_term(clean_lexicon, 'terms', df_submissions, 'full_text')\n","df_submissions['qtd_term'] = qtd_list"],"metadata":{"id":"XWBJZtn-7A43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Separar amostra de grupo de controle\n","df_sample, df_control = split_groups(df_submissions, 'qtd_term', 5)"],"metadata":{"id":"b6TZo5sY88La"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Criar lista de stop words\n","contextual_stopwords = ['pra', 'nao', 'ate', 'ja', 'etc', 'porem', 'ta', 'estao', 'pro', 'alguma', 'vao', 'voce', 'apos', 'ne', 'muita', 'mim', 'dela',\n","                 'literalmente', 'gabriel', 'entao', 'que', 'la', 'vez', 'coisa', 'carar', 'ai', 'tipo', 'mesmo', 'ha', 'atra', 'voce', 'fico', 'parte',\n","                  'ideiar', 'sao', 'mesmo', 'atra', 'faco', 'vejo',  'claro', 'ideiar']\n","stopwords = create_stopwords (df_sample, 'full_text', contextual_stopwords)"],"metadata":{"id":"sISq3F0F_qRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Retirar stopwords do texto\n","df_sample['clean_text'] = remove_stopwords(df_sample, 'full_text', stopwords)"],"metadata":{"id":"dNX3Klmp_sVM","executionInfo":{"status":"ok","timestamp":1693951852429,"user_tz":180,"elapsed":1105055,"user":{"displayName":"NICOLLY LIRA DE ALBUQUERQUE","userId":"13949517078806762460"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"65b1869d-06fb-4939-e5f3-7d5289f070a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-1ff86cf1ec07>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_sample['clean_text'] = remove_stopwords(df_sample, 'full_text', stopwords)\n"]}]},{"cell_type":"markdown","source":["# Salvar em csv"],"metadata":{"id":"J9OT_mQUGw3q"}},{"cell_type":"code","source":["df_sample.to_csv('/content/drive/MyDrive/tcc/bases_de_dados/bases_tratadas/base_preprocessada.csv', sep = ',', encoding = 'utf-8', index = False)"],"metadata":{"id":"rkKMWOqMGkPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"K2qkKYR2MmdH"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}